
# Hand Gesture Recognition
## Danny Lindberg and Nick Littlefield

### Overview
The problem that we face is creating a way for machines to understand certain hand gestures, using computer vision, in order for people to communicate with each other when one, or multiple members, is not able to communicate vocally. This can be handy in hospital emergency rooms, military situations and even, if the dataset was expanded, communicating with someone who only can only communicate through ASL. In this specific instance we focused on the gestures OK, thumbs up, thumbs down and a camera facing palm, to show proof of concept.


### Materials and Methods
#### Datasets
Two datasets were used to build different verisons. 

#### Leap Motion Dataset
Consists of black and white images of different hand gestures. We were only interested in the images for Okay, Thumb Up, and Palm. We extracted these images from the dataset to use in our project. The dataset didn't have any images for dislike, so these images were generated by flipping the thumbs up images. In total, there were 7,601 images. We split the dataset into a training/validation/test set using a 70/20/10 split. 

The entire dataset can be found here: 
https://www.kaggle.com/gti-upm/leapgestrecog

The images extracted and used in the project can be found here: https://drive.google.com/drive/folders/1wb9q1kvlX7IJt2JLXSBVOUDbX0vTEMNw

#### TinyHGR Dataset
The TinyHGR dataset has images for 7 different hand gestures done by 40 different participants. This dataset is large, so we extracted the four hand gestures were interested in: Okay, Thumb Up, and Palm. Once fully extracted we had 134, 292 images. This was too many image for us to train a model on in Google Colab, so we took a random subset of the data. This instead gave us 33,574 images to work with. We used a 60/25/15 split to generate the training/validation/test set. 

The entire dataset can be found here: 
https://sites.google.com/view/visionlearning/databases/image-database-for-tiny-hand-gesture-recognition

The images extracted and used in the project can be found here: https://drive.google.com/drive/folders/1wb9q1kvlX7IJt2JLXSBVOUDbX0vTEMNw

#### Libraries 
For this project we used the PyTorch, OpenCV, Pandas, and Numpy libraries. The entire project is built on top of pre-trained models that were downloaded from the PyTorch library. These include:
- ResNet18
- ResNet50
- Inception V3

These models were not fine-tuned to the dataset. Instead, we extracted the features from the models, and added new classification layers to the model. These classification layers were then trained. 

### Results

### Conclusions

### Discussion

### Outlooks
Moving forward we would train the ResNet and Inception models on the TinyHGR dataset further to get better results. After getting better results it could be used to try and capture hand gestures in real time, such as in a hospital setting. Along with this, we could add more gestures to the list to make the application more comprehensive. This would increase the variety of settings the application could be used in. 
