
# Hand Gesture Recognition
## Danny Lindberg and Nick Littlefield

### Overview
The problem that we face is creating a way for machines to understand certain hand gestures, using computer vision, in order for people to communicate with each other when one, or multiple members, is not able to communicate vocally. This can be handy in hospital emergency rooms, military situations and even, if the dataset was expanded, communicating with someone who only can only communicate through ASL. In this specific instance we focused on the gestures OK, thumbs up, thumbs down and a camera facing palm, to show proof of concept.

### Materials and Methods
#### Datasets
Two datasets were used to build different verisons. 

#### Leap Motion Dataset
Consists of black and white images of different hand gestures. We were only interested in the images for Okay, Thumb Up, and Palm. We extracted these images from the dataset to use in our project. The dataset didn't have any images for dislike, so these images were generated by flipping the thumbs up images. In total, there were 7,601 images. We split the dataset into a training/validation/test set using a 70/20/10 split. 

The entire dataset can be found here: 
https://www.kaggle.com/gti-upm/leapgestrecog

The images extracted and used in the project can be found here: https://drive.google.com/drive/folders/1wb9q1kvlX7IJt2JLXSBVOUDbX0vTEMNw

#### TinyHGR Dataset
The TinyHGR dataset has images for 7 different hand gestures done by 40 different participants. This dataset is large, so we extracted the four hand gestures were interested in: Okay, Thumb Up, and Palm. Once fully extracted we had 134, 292 images. This was too many image for us to train a model on in Google Colab, so we took a random subset of the data. This instead gave us 33,574 images to work with. We used a 60/25/15 split to generate the training/validation/test set. 

The entire dataset can be found here: 
https://sites.google.com/view/visionlearning/databases/image-database-for-tiny-hand-gesture-recognition

The images extracted and used in the project can be found here: https://drive.google.com/drive/folders/1wb9q1kvlX7IJt2JLXSBVOUDbX0vTEMNw

#### Libraries 
For this project we used the PyTorch, OpenCV, Pandas, and Numpy libraries. The entire project is built on top of pre-trained models that were downloaded from the PyTorch library. These include:
- ResNet18
- ResNet50
- Inception V3

These models were not fine-tuned to the dataset. Instead, we extracted the features from the models, and added new classification layers to the model. These classification layers were then trained. 

#### Transforms
The following transforms were used on the training dataset:
- Random Crop (Size 224 for Resnet, 299 for InceptionV3)
- Resize (Size 224 for ResNet, 299 for InceptionV3)
- Normalization

Normalization used the same ranges for all models. The means used were [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224, 0.225].

For the validation and testing data only resize and normalization were done.

### Results

#### LeapMotion Models

#### TinyHGR Models
##### ResNet50
The metrics for ResNet 50 are shown below:
```
              precision    recall  f1-score   support

           0      0.564     0.578     0.571       462
           1      0.625     0.840     0.716       525
           2      0.745     0.478     0.582       477
           3      0.591     0.568     0.579       551

    accuracy                          0.620      2015
   macro avg      0.631     0.616     0.612      2015
weighted avg      0.630     0.620     0.614      2015
```
The confusion matrix is:

![plot](/files/resnet50_tinyhgr_cm.png)

### Conclusions

### Discussion

There are some limitations to our project. These limitations include:
- Using the same learning rate and optimizers for all the models.  Better results could have been achieved it these were adjusted per model
- ResNet50 and InceptionV3 were limited to being trained for 2 epochs. 
- We aren't using entire datasets. Limiting the gestures the model can classify.
- Trained on balanced dataset. Model would not work well in an environment where the available images/gestures were imbalanced.

### Outlooks
Moving forward we would train the ResNet and Inception models on the TinyHGR dataset further to get better results. After getting better results it could be used to try and capture hand gestures in real time, such as in a hospital setting. Along with this, we could add more gestures to the list to make the application more comprehensive. This would increase the variety of settings the application could be used in. 
