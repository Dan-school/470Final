{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"basic_training_setup_v2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PU4Qu8xGombY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"USPwqPwaoRBW"},"source":["! unzip /content/drive/MyDrive/HandGestures/data/LeapMotion/imgs.zip\n","# ! unzip /content/drive/MyDrive/HandGestures/data/TinyHGRSubset/subset_imgs.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgXvG4aNSHH5"},"source":["import cv2\n","import numpy as np\n","import pandas as pd\n","\n","import torch \n","import torch.nn as nn\n","import torch.optim\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7soW12-qn1mu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfy5b48TSYeW"},"source":["# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","device = 'cpu'\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    device = 'cuda'\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEqfGMLSsQ27"},"source":["torch.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OJlaWAhbXs2"},"source":["labels = {\n","    \"ThumbUp\": 0,\n","    \"ThumbDown\": 1,\n","    \"Ok\": 2,\n","    \"Palm\": 3\n","}\n","\n","class LeapMotionDataset(torch.utils.data.Dataset):\n","  # Dataset for Leap Motion data\n","  def __init__(self, csv_file, file_col, lbl_col, text2lbl, transforms=None):\n","    \"\"\"\n","    Initialize Dataset\n","\n","    params:\n","      - csv_file: Path to CSV\n","      - file_col: Column corresponding to file name\n","      - lbl_col: Column corresponding to label column\n","      - text2lbl: Numerical representation of labels\n","      - transforms: Transforms to apply to imgs\n","    \"\"\"\n","\n","    self.csv = pd.read_csv(csv_file)\n","    self.file_col = file_col\n","    self.lbl_col = lbl_col\n","    self.labels = text2lbl\n","    self.transforms = transforms\n","\n","  def __len__(self):\n","    \"\"\"\n","    Returns the length of dataset\n","    \"\"\"\n","\n","    return len(self.csv)\n","\n","  def __getitem__(self, idx):\n","    \"\"\"\n","    Get item from dataset\n","\n","    params:\n","      - idx: Index of data record to get.\n","    \"\"\"\n","\n","    img = cv2.imread(self.csv.loc[idx][self.file_col])\n","\n","    # Thumb image have different channel orgainization that we need to account for\n","    # We also need to rearrange channels. PyTorch expect C x H x W\n","    if \"Thumb\" not in self.csv.loc[idx][self.lbl_col ]:\n","      img = img.astype(np.float).transpose(2, 0, 1)\n","    else:\n","      img = img.astype(np.float).transpose(2, 1, 0)\n","    label = self.csv.loc[idx][self.lbl_col]\n","\n","    # Convert to tensor and apply transforms\n","    img = torch.from_numpy(img)\n","    if self.transforms:\n","      img = self.transforms(img)\n","\n","    sample = {\"img\": img, \"label\": self.labels[label]}\n","\n","    return sample\n","\n","class TinyHGRDataset(torch.utils.data.Dataset):\n","  # Dataset for TinyHGR data\n","  def __init__(self, csv_file, file_col, lbl_col, text2lbl, transforms=None):\n","    \"\"\"\n","    Initialize Dataset\n","\n","    params:\n","      - csv_file: Path to CSV\n","      - file_col: Column corresponding to file name\n","      - lbl_col: Column corresponding to label column\n","      - text2lbl: Numerical representation of labels\n","      - transforms: Transforms to apply to imgs\n","    \"\"\"\n","    self.csv = pd.read_csv(csv_file)\n","    self.file_col = file_col\n","    self.lbl_col = lbl_col\n","    self.labels = text2lbl\n","    self.transforms = transforms\n","\n","  def __len__(self):\n","    \"\"\"\n","    Returns the length of dataset\n","    \"\"\"\n","\n","    return len(self.csv)\n","\n","  def __getitem__(self, idx):\n","    \"\"\"\n","    Get item from dataset\n","\n","    params:\n","      - idx: Index of data record to get.\n","    \"\"\"\n","    img = cv2.imread(self.csv.loc[idx][self.file_col])\n","    if img.shape[0] != 480 and img.shape[1] != 640:\n","      img = cv2.resize(img, (640, 480))\n","\n","    # Rearrange channels. PyTorch expect C x H x W\n","    img = img.astype(np.float).transpose(2, 0, 1)\n","    \n","    label = self.csv.loc[idx][self.lbl_col]\n","\n","    # Convert to tensor and apply transforms\n","    img = torch.from_numpy(img)\n","    if self.transforms:\n","      img = self.transforms(img)\n","\n","    sample = {\"img\": img, \"label\": self.labels[label]}\n","\n","    return sample"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uh5UqZRqxiEd"},"source":["def set_parameter_requires_grad(model, extract_features):\n","  \"\"\"\n","  Extract features from a pretrained model\n","\n","  params:\n","  model -- pretrained model\n","  extract_features -- True or False, indicating whether or not to extract features from model\n","  \"\"\"\n","  if extract_features:\n","    for param in model.parameters():\n","      param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsHy887qYk7H"},"source":["from torchvision.models import resnet18, resnet50, inception_v3\n","\n","def load_and_initalize(model_name, num_classes, extract_features=True):\n","  \"\"\"\n","  Load and initialize a pretrained model. Classification layer of models are set to be trained,\n","  while the rest of the model remains frozen. \n","\n","  params:\n","    - model_name: Pretrained model name to load\n","    - num_classes: Number of classes for the model\n","    - extract_features: Whether or not to extract features from the model\n","  \"\"\"\n","  input_size = 0\n","  model = None\n","\n","  if model_name == \"resnet18\":\n","    model = resnet18(pretrained=True)\n","\n","    # Extract model features\n","    set_parameter_requires_grad(model, extract_features)\n","\n","    # Set classification layers to fit number of classes for model\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    input_size = 224\n","  elif model_name == \"resnet50\":\n","    model = resnet50(pretrained=True)\n","\n","    # Extract model features\n","    set_parameter_requires_grad(model, extract_features)\n","\n","    # Set classification layers to fit number of classes for model\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    input_size = 224\n","  elif model_name == \"inception_v3\":\n","    model = inception_v3(pretrained=True)\n","    # Extract model features\n","    set_parameter_requires_grad(model, extract_features)\n","    \n","    # Handle the auxilary net\n","    num_ftrs = model.AuxLogits.fc.in_features\n","    model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","\n","    # Handle primary net\n","    num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(num_ftrs, num_classes)\n","    input_size = 299\n","\n","  return model, input_size\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1oqgBiCk-Eu"},"source":["def train_model(model, train_dl, optimizer, loss_fn, is_inception=False, valid_dl = None, epochs=10):\n","  \"\"\"\n","  Train a model\n","\n","  params:\n","    - model: Model to use for training\n","    - train_dl: DataLoader for training data\n","    - optimizer: Optimizer to use for training\n","    - loss_fn: Loss function for training\n","    - is_inception: Indicator for whether or not we are training an inception network.\n","    - valid_dl: Validation DataLoader (optional)\n","    - epochs: Number of epochs for training\n","\n","  returns:\n","    - train_loss: List of training losses\n","    - val_loss: list of validation losses\n","  \"\"\"\n","  model.to(device)\n","\n","  for ep in range(1, epochs + 1):\n","    train_loss, val_loss = [], []\n","    print(\"---\" * 3, \"Epoch\", ep, \"---\" * 3)\n","    model.train()\n","    for batch in train_dl:\n","      optimizer.zero_grad()\n","      img = batch[\"img\"].float().to(device)\n","      labels = batch[\"label\"].to(device)\n","      if is_inception:\n","        out, aux = model(img)\n","        loss_out = loss_fn(out, labels)\n","        loss_aux = loss_fn(aux, labels)\n","        loss = loss_out + 0.4*loss_aux\n","      else:\n","        out = model(img)\n","        loss = loss_fn(out, batch[\"label\"].to(device))\n","      train_loss.append(loss.item())\n","      loss.backward()\n","      optimizer.step()\n","    print(\"Training Loss: \", np.mean(train_loss))\n","    \n","    if valid_dl is not None:\n","      model = model.eval()\n","      for batch in valid_dl:\n","        img = batch[\"img\"].float().to(device)\n","        labels = batch[\"label\"].to(device)\n","        out = model(img)\n","        loss = loss_fn(out, batch[\"label\"].to(device))\n","        val_loss.append(loss.item())\n","      print(\"Validation Loss: \", np.mean(val_loss))\n","\n","  return train_loss, val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8mkyWfZo12P"},"source":["def test_model(model, testloader):\n","  \"\"\"\n","  Test a model\n","\n","  params:\n","    - model: Model to test\n","    - testloader: DataLoader for test set\n","\n","  returns: \n","    - y_true: true labels\n","    - y_pred: final predictions\n","  \"\"\"\n","  model.eval()\n","  y_true, y_pred = [], []\n","  for batch in testloader:\n","    img = batch[\"img\"].float().to(device)\n","    out = model(img)\n","    y_true.extend(batch[\"label\"])\n","    probabilities = torch.softmax(model(img), dim=1)\n","    predicted_class = torch.argmax(probabilities, dim=1)\n","    y_pred.extend(predicted_class.cpu())\n","  return y_true, y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sAMlRjLw4wT3"},"source":["# Modified: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Greens):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred, labels=classes)\n","\n","    print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jL1q4VcqSUc"},"source":["model, img_size = load_and_initalize(\"resnet50\", 4)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pACeB9px8Fe4"},"source":["transforms_list = transforms.Compose([transforms.Resize(img_size), \n","                                 transforms.RandomCrop(img_size),\n","                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_l7lXB5oKg-"},"source":["# Build Datasets\n","train_ds = LeapMotionDataset(\"/content/drive/MyDrive/HandGestures/data/LeapMotion/train_split.csv\",\n","                            \"file\", \"label\", labels, transforms=transforms_list)\n","valid_ds = LeapMotionDataset(\"/content/drive/MyDrive/HandGestures/data/LeapMotion/valid_split.csv\",\n","                            \"file\", \"label\", labels, transforms=transforms.Compose([transforms.Resize(img_size),\n","                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n","test_ds = LeapMotionDataset(\"/content/drive/MyDrive/HandGestures/data/LeapMotion/test_split.csv\",\n","                            \"file\", \"label\", labels, transforms=transforms.Compose([transforms.Resize(img_size),\n","                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NX-EFuFfoFaW"},"source":["# Build DataLoaders\n","trainloader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n","validloader = torch.utils.data.DataLoader(valid_ds, batch_size=16, shuffle=True)\n","testloader = torch.utils.data.DataLoader(test_ds, batch_size=16, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bqssgd00n4-e"},"source":["optimizer = torch.optim.SGD(model.parameters(), 0.001)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","train_loss, valid_loss = train_model(model, trainloader, optimizer, loss_fn, valid_dl=validloader, is_inception=False, epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9HEDwNDynWZ"},"source":["y_true, y_pred = test_model(model, testloader)\n","from sklearn.metrics import classification_report\n","print(classification_report(y_true, y_pred, digits=3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7Aa_G0Vni-O"},"source":["torch.save(model.state_dict(), \"/content/drive/MyDrive//HandGestures/models/resnet50_leapmotion.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCEHmw6ZEDlH"},"source":["class2lbl = {c: lbl for lbl, c in labels.items()}\n","y_true_lbls = [class2lbl[c.item()] for c in y_true]\n","y_pred_lbls = [class2lbl[c.item()] for c in y_pred]\n","\n","plot_confusion_matrix(y_true_lbls, y_pred_lbls, classes=list(labels.keys()), title=\"Confusion Matrix, ResNet50\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rzdx9HsEdcZ"},"source":[""],"execution_count":null,"outputs":[]}]}